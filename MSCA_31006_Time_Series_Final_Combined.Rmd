---
title: 'MSCA 31006: Final Project - AQI Forecasting'
author: "Aashish Singh, Jacob Brewer, Nikitha Gopal, Sydney Peters"
date: "8/10/2023"
output: html_document
---

```{r, tidy=TRUE}
#install.packages(c("forecast", "expsmooth", "seasonal")) 
library(TTR)
library(forecast) 
library(tseries) 
library(expsmooth) 
library(fpp2)
library(seasonal)
library(MASS)
library(stats)
library(TSA)
library(forecast)
library(ggplot2)
library(tseries)
library(imputeTS)
library(vars)
library(timetk)
library(Metrics)
library(lmtest)
library(lubridate)
library(xts)
library(dplyr)
library(tidyr)
library(sqldf)
```

#### Step 1: Load data and plot the time series

```{r, tidy=TRUE}
path <- "/Users/aashishsingh/Desktop/Time Series - Final Project/"
pm2_5_data <- read.csv(paste0(path, "la_pm2_5_pollutant_data.csv"), 
                     na.strings=c("", "NA"))
ozone_data <- read.csv(paste0(path, "la_ozone_pollutant_data.csv"), 
                     na.strings=c("", "NA"))

# Convert data into ts objects
pm2_5_ts <- ts(pm2_5_data$PM2.5.AQI.Value, start = c(1999,1,1), frequency = 365.25)
ozone_ts <- ts(ozone_data$Ozone.AQI.Value, start = c(1999,1,1), frequency = 365.25)

plot(pm2_5_ts, main="Los Angeles PM 2.5 AQI")
plot(ozone_ts, main="Los Angeles Ozone AQI")
```

#### Step 2: Check if data is stationary (KPSS/ADF Test)

```{r, tidy=TRUE}

kpss.test(pm2_5_ts)
# The reported p-value is 0.01, which is smaller than 0.05, and would suggest 
# that we reject the null hypothesis of level stationarity and conclude that 
# there is evidence that the data is non-stationary

adf.test(pm2_5_ts)
# Similarly the Augumented Dickey-Fuller test results in p-value of 0.01 (<0.05) 
# where we do reject the null hypothesis that the time series is non-stationary
# and thus data is stationary.

# These are opposite results so we use ACF PACF plots to see if the series is stationary
acf(pm2_5_ts, main = "ACF of Original Time Series")
pacf(pm2_5_ts, main = "PACF of Original Time Series")

```
#### Step 3: Prepare data for Weekly & Monthly extraction and EDA

```{r, tidy=TRUE}
# Create data with Dates
pm2_5_df <- as.data.frame(pm2_5_ts)
pm2_5_df$Date <- mdy(pm2_5_data$Date)
colnames(pm2_5_df) <- c("PM2.5.AQI.Value", "Date")

# Convert to xts format for weekly & monthly
pm2_5_xts <- as.xts(pm2_5_df, order.by=pm2_5_df$Date)
pm2_5_xts <- pm2_5_xts[,-2]
```

#### Step 4: Understand PM2.5 AQI variance across years

```{r, tidy=TRUE}
# Let's see how seasonality looks over time and is the variance changing
pm2_5_df$Month <- month(pm2_5_df$Date)
pm2_5_df$Year <- year(pm2_5_df$Date)

avg_aqi_by_month_year <- pm2_5_df %>%
  group_by(pm2_5_df$Year, pm2_5_df$Month) %>%
  summarise(
    avg_value = mean(PM2.5.AQI.Value)
  )
colnames(avg_aqi_by_month_year) <- c("Year", "Month", "avg_value")

reshape_avg_aqi_by_month_year <- 
  sqldf(
    "SELECT 
      Month,
      MAX(CASE WHEN Year = 1999 THEN avg_value END) AS Year_1999,
      MAX(CASE WHEN Year = 2000 THEN avg_value END) AS Year_2000,
      MAX(CASE WHEN Year = 2001 THEN avg_value END) AS Year_2001,
      MAX(CASE WHEN Year = 2002 THEN avg_value END) AS Year_2002,
      MAX(CASE WHEN Year = 2003 THEN avg_value END) AS Year_2003,
      MAX(CASE WHEN Year = 2004 THEN avg_value END) AS Year_2004,
      MAX(CASE WHEN Year = 2005 THEN avg_value END) AS Year_2005,
      MAX(CASE WHEN Year = 2006 THEN avg_value END) AS Year_2006,
      MAX(CASE WHEN Year = 2007 THEN avg_value END) AS Year_2007,
      MAX(CASE WHEN Year = 2008 THEN avg_value END) AS Year_2008,
      MAX(CASE WHEN Year = 2009 THEN avg_value END) AS Year_2009,
      MAX(CASE WHEN Year = 2010 THEN avg_value END) AS Year_2010,
      MAX(CASE WHEN Year = 2011 THEN avg_value END) AS Year_2011,
      MAX(CASE WHEN Year = 2012 THEN avg_value END) AS Year_2012,
      MAX(CASE WHEN Year = 2013 THEN avg_value END) AS Year_2013,
      MAX(CASE WHEN Year = 2014 THEN avg_value END) AS Year_2014,
      MAX(CASE WHEN Year = 2015 THEN avg_value END) AS Year_2015,
      MAX(CASE WHEN Year = 2016 THEN avg_value END) AS Year_2016,
      MAX(CASE WHEN Year = 2017 THEN avg_value END) AS Year_2017,
      MAX(CASE WHEN Year = 2018 THEN avg_value END) AS Year_2018,
      MAX(CASE WHEN Year = 2019 THEN avg_value END) AS Year_2019,
      MAX(CASE WHEN Year = 2020 THEN avg_value END) AS Year_2020,
      MAX(CASE WHEN Year = 2021 THEN avg_value END) AS Year_2021,
      MAX(CASE WHEN Year = 2022 THEN avg_value END) AS Year_2022,
      MAX(CASE WHEN Year = 2023 THEN avg_value END) AS Year_2023
    FROM avg_aqi_by_month_year
    GROUP BY Month
    ORDER BY Month"
  )

colnames(reshape_avg_aqi_by_month_year) <- c(
  "Month", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", 
  "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", 
  "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023")

boxplot(reshape_avg_aqi_by_month_year[2:26])

# We can see that over the years there is downward trend and variance is decreasing
# except 2020 when we see a high peak likely caused by wildfires
# Link: https://en.wikipedia.org/wiki/Lake_Fire_(2020)
```

#### Step 5a: Extract values for time series and plot the series - Weekly

```{r, tidy=TRUE}
pm2_5_weekly <- apply.weekly(pm2_5_xts, mean)
pm2_5_weekly_ts <- ts(pm2_5_weekly["19990103/20230811"], 
                      start = c(1999,1),
                      frequency = 52.18)
plot(pm2_5_weekly_ts)
# Strong seasonality, Strong cyclic, light downward trend, variance is reducing

kpss.test(pm2_5_weekly_ts)

adf.test(pm2_5_weekly_ts)

acf(pm2_5_weekly_ts, main = "ACF of Weekly Time Series")
pacf(pm2_5_weekly_ts, main = "PACF of Weekly Time Series")

```

#### Step 6: Split data into train and test - Weekly

```{r, tidy=TRUE}
# Split the data into a training and test dataset
train_weekly <- window(pm2_5_weekly_ts, start = c(1999,1), end=c(2020,52))
test_weekly <- window(pm2_5_weekly_ts, start = c(2021,1))
```

#### Step 7: Decompose the time series plot - Weekly

```{r, tidy=TRUE}
# Looking at spectral analysis
periodogram(train_weekly, log = "no", plot=TRUE, 
            ylab = "Periodogram",
            xlab = "Frequency",
            lwd=2, xlim = c(0, 0.06))

# There are two trends: 
#  1) A typical yearly (52 weeks) seasonal trend
#  2) A trend that is repeating every 9.6 years (500 weeks)
#  3) A typical half yearly (26 weeks) seasonal trend

# Overall, there is no mixed effect that normal seasonality model cannot capture.

# Decompose the series
plot(decompose(train_weekly, type="multiplicative"))

# Important Inferences
# 1) The PM2.5 AQI has been decreasing overall though there are rise every now and then.
#    However the trend is going down with time.
# 2) Winter months have a strong seasonal effect with Nov and Dec being the peak months
#    usually which likely could be due to cold temperatures causing pollutant to
#    not escape the lower atmosphere.
#    Link: https://www.accuweather.com/en/health-wellness/why-air-pollution-is-worse-in-winter/689434#:~:text=Cold%20air%20is%20denser%20and%20moves%20slower%20than%20warm%20air,rate%20than%20during%20the%20summer.
# 3) We can see a seasonal cycle of 12 months where the mean value of each month starts 
#    with a increasing trend in the beginning of the year and drops down towards 
#    the end of the year. We can see a seasonal effect with a cycle of 12 months.


# Understand the seasonality and remove it to see the trend
train_weekly_diff <- diff(train_weekly, lag = 52)
autoplot(train_weekly_diff, ylab="Train Seasonal Differencing - Weekly Data")

# Let's look if the series is stationary?
kpss.test(train_weekly_diff)
# The reported p-value is 0.1, which is > 0.05, and would suggest that we fail 
# to reject the null hypothesis of level stationarity (conclusion: stationary)

adf.test(train_weekly_diff)
# The reported p-value of 0.01 (<0.05) so we do reject the null hypothesis that 
# the time series is non-stationary (conclusion: stationary)

acf(train_weekly_diff, main = "ACF of Seasonal Differencing Time Series - Weekly Data")
pacf(train_weekly_diff, main = "PACF of Seasonal Differencing Time Series - Weekly Data")

```

#### Step 8: Benchmark using snaive model - Weekly

```{r, tidy=TRUE}
# Forecast the seasonal naÃ¯ve for (2021-01 to 2023-07)
h <- length(test_weekly)
forecast_snaive_weekly <- snaive(train_weekly, lambda="auto", h)
summary(forecast_snaive_weekly)
# Plot the forecasts for snaive model
plot(forecast_snaive_weekly, main = "PM 2.5 AQI Forecast - SNaive (Weekly)",
         xlab = "Week", ylab = "PM 2.5 AQI")
lines(test_weekly)

# Compare the forecast with the actual test data by calculating MAPE and MSE
# Symmetric Mean Absolute Percentage Error (sMAPE)
smape_snaive_weekly <- smape(test_weekly, forecast_snaive_weekly$mean)
smape_snaive_weekly

# Mean Absolute Error (MAE)
mae_snaive_weekly <- mean(abs((test_weekly - forecast_snaive_weekly$mean)))
mae_snaive_weekly

# Root Mean Squared Error (RMSE)
rmse_snaive_weekly <- rmse(test_weekly, forecast_snaive_weekly$mean)
rmse_snaive_weekly

# Evaluate the residuals
checkresiduals(forecast_snaive_weekly)

```

#### Step 9: Forecast using SARIMA - Weekly

```{r}
eacf <-
function (z,ar.max=7,ma.max=13) 
{
#
#  PROGRAMMED BY K.S. CHAN, DEPARTMENT OF STATISTICS AND ACTUARIAL SCIENCE,
#  UNIVERSITY OF IOWA.
#
#  DATE: 4/2001
#  Compute the extended sample acf (ESACF) for the time series stored in z.
#  The matrix of ESACF with the AR order up to ar.max and the MA order
#  up to ma.max is stored in the matrix EACFM.
#  The default values for NAR and NMA are 7 and 13 respectively.
#  Side effect of the eacf function:
#  The function prints a coded ESACF table with
#  significant values denoted by * and nosignificant values by 0, significance
#  level being 5%.
#
#  Output:
#   eacf=matrix of esacf
#   symbol=matrix of coded esacf
#

lag1<-function(z,lag=1){c(rep(NA,lag),z[1:(length(z)-lag)])}
reupm<-function(m1,nrow,ncol){
k<-ncol-1
m2<-NULL
for (i in 1:k){
i1<-i+1
work<-lag1(m1[,i])
work[1]<--1
temp<-m1[,i1]-work*m1[i1,i1]/m1[i,i]
temp[i1]<-0
m2<-cbind(m2,temp)
}
m2}
ceascf<-function(m,cov1,nar,ncol,count,ncov,z,zm){
result<-0*seq(1,nar+1)
result[1]<-cov1[ncov+count]
for (i in 1:nar) {
temp<-cbind(z[-(1:i)],zm[-(1:i),1:i])%*%c(1,-m[1:i,i])
result[i+1]<-acf(temp,plot=FALSE,lag.max=count,drop.lag.0=FALSE)$acf[count+1]
}
result
}

ar.max<-ar.max+1
ma.max<-ma.max+1
nar<-ar.max-1
nma<-ma.max
ncov<-nar+nma+2
nrow<-nar+nma+1
ncol<-nrow-1
z<-z-mean(z)
zm<-NULL
for(i in 1:nar) zm<-cbind(zm,lag1(z,lag=i))
cov1<-acf(z,lag.max=ncov,plot=FALSE,drop.lag.0=FALSE)$acf
cov1<-c(rev(cov1[-1]),cov1)
ncov<-ncov+1
m1<-matrix(0,ncol=ncol,nrow=nrow)
for(i in 1:ncol) m1[1:i,i]<-
ar.ols(z,order.max=i,aic=FALSE,demean=FALSE,intercept=FALSE)$ar
eacfm<-NULL
for (i in 1:nma) {
m2<-reupm(m1=m1,nrow=nrow,ncol=ncol)
ncol<-ncol-1
eacfm<-cbind(eacfm, ceascf(m2,cov1,nar,ncol,i,ncov,z,zm))
m1<-m2}
work<-1:(nar+1)
work<-length(z)-work+1
symbol<-NULL
for ( i in 1:nma) {
work<-work-1
symbol<-cbind(symbol,ifelse(abs(eacfm[,i])>2/work^.5, 'x','o'))}
rownames(symbol)<-0:(ar.max-1)
colnames(symbol)<-0:(ma.max-1)
cat('AR/MA\n')
print(symbol,quote=FALSE)
invisible(list(eacf=eacfm,ar.max=ar.max,ma.ma=ma.max,symbol=symbol))
}
```

```{r}
eacf(train_weekly)
# p <- 1,2,3
# q <- 2,3,4
```

```{r, tidy=TRUE}
# Forecast the TBATS for (2021-01 to 2023-07)
h <- length(test_weekly)


model_sarima_weekly <- auto.arima(train_weekly, 
                                  seasonal = TRUE, 
                                  lambda="auto",
                                  start.p = 1, 
                                  max.p = 3,
                                  start.q = 2, 
                                  max.q = 4)

forecast_sarima_weekly <- forecast(model_sarima_weekly, h)
plot(forecast_sarima_weekly)

# Plot the forecasts for TBATS model
plot(pm2_5_weekly_ts, 
     xlim=c(1999, 2023), 
     ylim=c(min(pm2_5_weekly_ts), 
            max(pm2_5_weekly_ts)), 
     main="Train, Test, and Forecasted Data - Weekly", 
     xlab="Year", 
     ylab="PM2.5 AQI Value")
lines(test_weekly, col="red") # Test data in red
lines(forecast_sarima_weekly$mean, col="blue", type="o") # Forecasted data in blue
legend("topleft", 
       legend=c("Train", "Test", "Forecast"), 
       fill=c("black", "red", "blue"))

# Compare the forecast with the actual test data by calculating MAPE and MSE
# Symmetric Mean Absolute Percentage Error (sMAPE)
smape_sarima_weekly <- smape(test_weekly, forecast_sarima_weekly$mean)
smape_sarima_weekly

# Mean Absolute Error (MAE)
mae_sarima_weekly <- mean(abs((test_weekly - forecast_sarima_weekly$mean)))
mae_sarima_weekly

# Root Mean Squared Error (RMSE)
rmse_sarima_weekly <- rmse(test_weekly, forecast_sarima_weekly$mean)
rmse_sarima_weekly

# Evaluate the residuals
checkresiduals(forecast_sarima_weekly)

comparison_df <- data.frame(
  Date = time(test_weekly),
  Actual = as.vector(test_weekly),
  Forecasted = as.vector(forecast_sarima_weekly$mean)
)
print(comparison_df)

```

#### Step 10: Forecast using TBATS - Weekly

```{r, tidy=TRUE}
# Forecast the TBATS for (2021-01 to 2023-07)
h <- length(test_weekly)

vec <- as.vector(train_weekly)
univariate_ts <- ts(vec, start=1999, frequency=52.18)
plot(stl(univariate_ts, s.window="periodic"))


model_tbats_weekly <- tbats(train_weekly,
                            use.box.cox = TRUE)
forecast_tbats_weekly <- forecast(model_tbats_weekly, h)
plot(forecast_tbats_weekly)

# Plot the forecasts for TBATS model
plot(pm2_5_weekly_ts, 
     xlim=c(1999, 2023), 
     ylim=c(min(pm2_5_weekly_ts), 
            max(pm2_5_weekly_ts)), 
     main="Train, Test, and Forecasted Data - Weekly", 
     xlab="Year", 
     ylab="PM2.5 AQI Value")
lines(test_weekly, col="red") # Test data in red
lines(forecast_tbats_weekly$mean, col="blue", type="o") # Forecasted data in blue
legend("topleft", 
       legend=c("Train", "Test", "Forecast"), 
       fill=c("black", "red", "blue"))

# Compare the forecast with the actual test data by calculating MAPE and MSE
# Symmetric Mean Absolute Percentage Error (sMAPE)
smape_tbats_weekly <- smape(test_weekly, forecast_tbats_weekly$mean)
smape_tbats_weekly

# Mean Absolute Error (MAE)
mae_tbats_weekly <- mean(abs((test_weekly - forecast_tbats_weekly$mean)))
mae_tbats_weekly

# Root Mean Squared Error (RMSE)
rmse_tbats_weekly <- rmse(test_weekly, forecast_tbats_weekly$mean)
rmse_tbats_weekly

# Evaluate the residuals
checkresiduals(forecast_tbats_weekly)

comparison_df <- data.frame(
  Date = time(test_weekly),
  Actual = as.vector(test_weekly),
  Forecasted = as.vector(forecast_tbats_weekly$mean)
)
print(comparison_df)

```


#### Step 11: Forecast using Neural Network - Weekly

```{r, tidy=TRUE}
h <- length(test_weekly)
model_nn_weekly <- nnetar(train_weekly,
                           size=10,
                           decay=0.5,
                           repeats=1000,
                           lambda="auto")
summary(model_nn_weekly)
forecast_nn_weekly <- forecast(model_nn_weekly, PI=FALSE, h)
plot(forecast_nn_weekly)

# Plot the forecasts for NN model
plot(pm2_5_weekly_ts, 
     xlim=c(1999, 2023), 
     ylim=c(min(pm2_5_weekly_ts), 
            max(pm2_5_weekly_ts)), 
     main="Train, Test, and Forecasted Data - Weekly", 
     xlab="Year", 
     ylab="PM2.5 AQI Value")
lines(test_weekly, col="red") # Test data in red
lines(forecast_nn_weekly$mean, col="blue", type="o") # Forecasted data in blue
legend("topleft", 
       legend=c("Train", "Test", "Forecast"), 
       fill=c("black", "red", "blue"))

# Compare the forecast with the actual test data by calculating MAPE and MSE
# Symmetric Mean Absolute Percentage Error (sMAPE)
smape_nn_weekly <- smape(test_weekly, forecast_nn_weekly$mean)
smape_nn_weekly

# Mean Absolute Error (MAE)
mae_nn_weekly <- mean(abs((test_weekly - forecast_nn_weekly$mean)))
mae_nn_weekly

# Root Mean Squared Error (RMSE)
rmse_nn_weekly <- rmse(test_weekly, forecast_nn_weekly$mean)
rmse_nn_weekly

# Evaluate the residuals
checkresiduals(forecast_nn_weekly)

comparison_df <- data.frame(
  Date = time(test_weekly),
  Actual = as.vector(test_weekly),
  Forecasted = as.vector(forecast_nn_weekly$mean)
)
print(comparison_df)

```


